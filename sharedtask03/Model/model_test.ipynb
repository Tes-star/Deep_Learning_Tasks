{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras import Sequential\n",
    "from keras.activations import sigmoid, tanh, elu, selu\n",
    "from keras.initializers.initializers_v2 import GlorotNormal, HeNormal\n",
    "from keras.utils import to_categorical\n",
    "from sklearn import model_selection\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, mean_squared_error\n",
    "from tensorflow.python.ops.init_ops_v2 import lecun_normal\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.metrics import Precision, Recall\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, Embedding, LSTM, Bidirectional, TimeDistributed, RepeatVector, \\\n",
    "    GRU, RNN, SimpleRNN, BatchNormalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sharedtask02.Model.model import load_data, create_traindataset\n",
    "\n",
    "train, sampleTest, sampleSubmission = load_data()\n",
    "x_train, y_train, x_test, y_test = create_traindataset(train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_proportion=0.1\n",
    "dataset_size = int(x_train.shape[0] * data_proportion)\n",
    "if dataset_size > 0:\n",
    "    x_train = x_train[:dataset_size]\n",
    "    y_train = y_train[:dataset_size]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from keras.activations import linear\n",
    "from keras import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(90, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.15130886881436242))\n",
    "\n",
    "model.add(Bidirectional(LSTM(activation=selu,units=20, return_sequences=True)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.15130886881436242))\n",
    "\n",
    "model.add(GRU(activation=selu,units=10, return_sequences=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.15130886881436242))\n",
    "\n",
    "model.add(Dense(units=1, activation=linear))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_9 (Batc  (None, 90, 1)            4         \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 90, 1)             0         \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 90, 40)           3520      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 90, 40)           160       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 90, 40)            0         \n",
      "                                                                 \n",
      " gru_6 (GRU)                 (None, 10)                1560      \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 10)               40        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,295\n",
      "Trainable params: 5,193\n",
      "Non-trainable params: 102\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 103.7561 - mean_squared_error: 103.7561 - val_loss: 44.9549 - val_mean_squared_error: 44.9549\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 84.0834 - mean_squared_error: 84.0834 - val_loss: 36.8864 - val_mean_squared_error: 36.8864\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 77.1291 - mean_squared_error: 77.1291 - val_loss: 34.1746 - val_mean_squared_error: 34.1746\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 74.0768 - mean_squared_error: 74.0768 - val_loss: 30.5784 - val_mean_squared_error: 30.5784\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 71.8739 - mean_squared_error: 71.8739 - val_loss: 28.4340 - val_mean_squared_error: 28.4340\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 69.7191 - mean_squared_error: 69.7191 - val_loss: 27.6625 - val_mean_squared_error: 27.6625\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 67.6617 - mean_squared_error: 67.6617 - val_loss: 27.2443 - val_mean_squared_error: 27.2443\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 65.0844 - mean_squared_error: 65.0844 - val_loss: 26.4735 - val_mean_squared_error: 26.4735\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 62.8060 - mean_squared_error: 62.8060 - val_loss: 25.5530 - val_mean_squared_error: 25.5530\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 60.7002 - mean_squared_error: 60.7002 - val_loss: 24.4047 - val_mean_squared_error: 24.4047\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 58.6122 - mean_squared_error: 58.6122 - val_loss: 23.3466 - val_mean_squared_error: 23.3466\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 56.2477 - mean_squared_error: 56.2477 - val_loss: 22.3106 - val_mean_squared_error: 22.3106\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 54.3717 - mean_squared_error: 54.3717 - val_loss: 21.2930 - val_mean_squared_error: 21.2930\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 52.2021 - mean_squared_error: 52.2021 - val_loss: 20.3311 - val_mean_squared_error: 20.3311\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 50.3910 - mean_squared_error: 50.3910 - val_loss: 19.5409 - val_mean_squared_error: 19.5409\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 48.0153 - mean_squared_error: 48.0153 - val_loss: 18.8746 - val_mean_squared_error: 18.8746\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 45.8557 - mean_squared_error: 45.8557 - val_loss: 18.3302 - val_mean_squared_error: 18.3302\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 44.8689 - mean_squared_error: 44.8689 - val_loss: 17.8241 - val_mean_squared_error: 17.8241\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 42.9824 - mean_squared_error: 42.9824 - val_loss: 17.3926 - val_mean_squared_error: 17.3926\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 41.3221 - mean_squared_error: 41.3221 - val_loss: 17.0136 - val_mean_squared_error: 17.0136\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 39.5428 - mean_squared_error: 39.5428 - val_loss: 16.6760 - val_mean_squared_error: 16.6760\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 38.0100 - mean_squared_error: 38.0100 - val_loss: 16.3757 - val_mean_squared_error: 16.3757\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 36.6204 - mean_squared_error: 36.6204 - val_loss: 16.0800 - val_mean_squared_error: 16.0800\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 35.1613 - mean_squared_error: 35.1613 - val_loss: 15.7914 - val_mean_squared_error: 15.7914\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 33.8899 - mean_squared_error: 33.8899 - val_loss: 15.5577 - val_mean_squared_error: 15.5577\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 32.6160 - mean_squared_error: 32.6160 - val_loss: 15.4839 - val_mean_squared_error: 15.4839\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 31.6040 - mean_squared_error: 31.6040 - val_loss: 15.6054 - val_mean_squared_error: 15.6054\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 30.8750 - mean_squared_error: 30.8750 - val_loss: 15.8268 - val_mean_squared_error: 15.8268\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 29.5868 - mean_squared_error: 29.5868 - val_loss: 15.8792 - val_mean_squared_error: 15.8792\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 28.5676 - mean_squared_error: 28.5676 - val_loss: 15.8868 - val_mean_squared_error: 15.8868\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 27.6144 - mean_squared_error: 27.6144 - val_loss: 16.4572 - val_mean_squared_error: 16.4572\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 26.9277 - mean_squared_error: 26.9277 - val_loss: 17.9967 - val_mean_squared_error: 17.9967\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 26.2269 - mean_squared_error: 26.2269 - val_loss: 16.5448 - val_mean_squared_error: 16.5448\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 25.1318 - mean_squared_error: 25.1318 - val_loss: 16.9581 - val_mean_squared_error: 16.9581\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 24.3353 - mean_squared_error: 24.3353 - val_loss: 19.8820 - val_mean_squared_error: 19.8820\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 25.1541 - mean_squared_error: 25.1541 - val_loss: 16.8228 - val_mean_squared_error: 16.8228\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 23.3957 - mean_squared_error: 23.3957 - val_loss: 16.6649 - val_mean_squared_error: 16.6649\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 23.0423 - mean_squared_error: 23.0423 - val_loss: 19.8861 - val_mean_squared_error: 19.8861\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 22.2948 - mean_squared_error: 22.2948 - val_loss: 47.3910 - val_mean_squared_error: 47.3910\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 21.4284 - mean_squared_error: 21.4284 - val_loss: 121.7945 - val_mean_squared_error: 121.7945\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 21.3861 - mean_squared_error: 21.3861 - val_loss: 205.0638 - val_mean_squared_error: 205.0638\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 21.0376 - mean_squared_error: 21.0376 - val_loss: 371.7405 - val_mean_squared_error: 371.7405\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 20.1607 - mean_squared_error: 20.1607 - val_loss: 873.3997 - val_mean_squared_error: 873.3997\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 19.8875 - mean_squared_error: 19.8875 - val_loss: 1568.4277 - val_mean_squared_error: 1568.4277\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 19.5219 - mean_squared_error: 19.5219 - val_loss: 1289.0553 - val_mean_squared_error: 1289.0553\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 19.2773 - mean_squared_error: 19.2773 - val_loss: 664.7428 - val_mean_squared_error: 664.7428\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x240c069a440>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.optimizers import Adamax\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "        monitor='val_mean_squared_error',\n",
    "        min_delta=0.00,  # minimium amount of change to count as an improvement\n",
    "        patience=20,  # how many epochs to wait before stopping\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "model.compile(optimizer=Adamax(learning_rate=0.01597012843639325), loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1000, batch_size=8485, verbose=1,\n",
    "validation_data=(x_test, y_test),\n",
    "              callbacks=[early_stopping]\n",
    "              )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}