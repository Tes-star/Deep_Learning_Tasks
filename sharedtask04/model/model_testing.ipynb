{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\JetBrains\\PyCharm 2022.2.2\\plugins\\python\\helpers\\pydev\\_pydevd_bundle\\pydevd_utils.py:606: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for item in s.iteritems():\n",
      "C:\\Program Files\\JetBrains\\PyCharm 2022.2.2\\plugins\\python\\helpers\\pydev\\_pydevd_bundle\\pydevd_utils.py:606: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for item in s.iteritems():\n",
      "C:\\Program Files\\JetBrains\\PyCharm 2022.2.2\\plugins\\python\\helpers\\pydev\\_pydevd_bundle\\pydevd_utils.py:606: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for item in s.iteritems():\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [16], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m data_file\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m tokenlist \u001B[38;5;129;01min\u001B[39;00m parse_incr(data_file, fields\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNr\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWort\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTyp1\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTyp2\u001B[39m\u001B[38;5;124m\"\u001B[39m]):\n\u001B[1;32m----> 6\u001B[0m     \u001B[38;5;28;43mprint\u001B[39;49m(tokenlist)\n",
      "Cell \u001B[1;32mIn [16], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m data_file\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m tokenlist \u001B[38;5;129;01min\u001B[39;00m parse_incr(data_file, fields\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNr\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWort\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTyp1\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTyp2\u001B[39m\u001B[38;5;124m\"\u001B[39m]):\n\u001B[1;32m----> 6\u001B[0m     \u001B[38;5;28;43mprint\u001B[39;49m(tokenlist)\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:1179\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:620\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:929\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:920\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx:317\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.2.2\\plugins\\python\\helpers\\pydev\\pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.2.2\\plugins\\python\\helpers\\pydev\\pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1175\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from conllu import parse_incr\n",
    "from io import open\n",
    "data_file = open(\"../data/01_train/train.tsv\",'r', encoding='utf-8')\n",
    "\n",
    "for tokenlist in parse_incr(data_file, fields=[\"Nr\",\"Wort\",\"Typ1\",\"Typ2\"]):\n",
    "    print(tokenlist)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "<_io.TextIOWrapper name='../data/01_train/train.tsv' mode='r' encoding='utf-8'>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "df_train=pd.read_csv('../data/01_train/train.tsv', sep='\\t',\n",
    "            header=0,\n",
    "            names = [\"Nr\",\"Wort\",\"Typ1\",\"Typ2\"],\n",
    "            quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "\n",
    "\n",
    " #quoting=0, doublequote=False)\n",
    "            #quoting=csv.QUOTE_ALL)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "df_train = df_train.drop(df_train[df_train['Nr']=='#'].index)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "O             410764\nB-LOC           8281\nB-PER           7679\nB-ORG           5255\nI-PER           4491\nI-ORG           3770\nI-OTH           3507\nB-OTH           3024\nB-LOCderiv      2808\nI-LOC           1169\nB-ORGpart        805\nB-LOCpart        513\nB-OTHderiv       236\nB-OTHpart        190\nB-PERpart        184\nB-PERderiv        62\nB-ORGderiv        41\nI-OTHpart         20\nI-LOCderiv        19\nI-ORGpart         18\nI-OTHderiv         9\nI-PERpart          6\nI-LOCpart          1\nI-PERderiv         1\nName: Typ1, dtype: int64"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Typ1'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "O        410764\nB-LOC     11602\nB-PER      7925\nB-ORG      6101\nI-PER      4498\nI-ORG      3788\nI-OTH      3536\nB-OTH      3450\nI-LOC      1189\nName: Typ1, dtype: int64"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reformat labels\n",
    "labels_org=['B-LOCderiv','B-LOCpart',]\n",
    "\n",
    "labels_extension=['deriv','part']\n",
    "\n",
    "df_train.Typ1=df_train.Typ1.str.rstrip('deriv')\n",
    "df_train.Typ1=df_train.Typ1.str.rstrip('part')\n",
    "\n",
    "df_train['Typ1'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "O             450132\nB-LOC           1138\nB-LOCderiv       602\nB-PER            372\nB-ORG            222\nI-PER            123\nI-LOC             72\nB-OTH             49\nI-ORG             40\nB-LOCpart         33\nB-PERpart         22\nI-OTH             15\nB-PERderiv        15\nB-ORGpart          7\nI-LOCderiv         4\nB-ORGderiv         3\nB-OTHderiv         3\nB-OTHpart          1\nName: Typ2, dtype: int64"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_train['Typ2'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "        Nr          Wort   Typ1 Typ2\n0        1      Schartau  B-PER    O\n1        2         sagte      O    O\n2        3           dem      O    O\n3        4             \"      O    O\n4        5  Tagesspiegel  B-ORG    O\n...     ..           ...    ...  ...\n476847  19          lang      O    O\n476848  20       sperren      O    O\n476849  21            zu      O    O\n476850  22        lassen      O    O\n476851  23             .      O    O\n\n[452853 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Nr</th>\n      <th>Wort</th>\n      <th>Typ1</th>\n      <th>Typ2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Schartau</td>\n      <td>B-PER</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>sagte</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>dem</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>\"</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Tagesspiegel</td>\n      <td>B-ORG</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>476847</th>\n      <td>19</td>\n      <td>lang</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>476848</th>\n      <td>20</td>\n      <td>sperren</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>476849</th>\n      <td>21</td>\n      <td>zu</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>476850</th>\n      <td>22</td>\n      <td>lassen</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>476851</th>\n      <td>23</td>\n      <td>.</td>\n      <td>O</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n<p>452853 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "1     1.000000\n2     1.000000\n3     1.000000\n4     0.999875\n5     0.998542\n6     0.993250\n7     0.980042\n8     0.959333\n9     0.927875\n10    0.889833\n11    0.844333\n12    0.799000\n13    0.751250\n14    0.702333\n15    0.652958\n16    0.605000\n17    0.555083\n18    0.507375\n19    0.460833\n20    0.417417\n21    0.376417\n22    0.337333\n23    0.300208\n24    0.264750\n25    0.234583\n26    0.205167\n27    0.180167\n28    0.159333\n29    0.138000\n30    0.117792\n31    0.100542\n32    0.084583\n33    0.071000\n34    0.058417\n35    0.046417\n36    0.037083\n37    0.030250\n38    0.022917\n39    0.017458\n40    0.013667\n41    0.009333\n42    0.006583\n43    0.004375\n44    0.002542\n45    0.001708\n46    0.001167\n47    0.001042\n48    0.000667\n49    0.000458\n50    0.000208\n51    0.000125\n52    0.000083\n53    0.000042\n54    0.000042\n55    0.000042\n56    0.000042\nName: Nr, dtype: float64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Nr'].value_counts()/df_train['Nr'].value_counts()[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [w_1, w_2]\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>w_1</th>\n      <th>w_2</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length=10\n",
    "\n",
    "x_train=pd.DataFrame(columns=['w_1','w_2'])\n",
    "\n",
    "x_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\transformers_keras_dataloader\\scripts\\util\\load_tokenizer_and_model.py:49: UserWarning: Ignoring value set for use_gpu=True, since GPU isn't available.\n",
      "  warnings.warn(\"Ignoring value set for use_gpu=True, since GPU isn't available.\")\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x81 in position 317: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [22], line 9\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers_keras_dataloader\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m WordEmbedder\n\u001B[0;32m      5\u001B[0m tokenizer, model \u001B[38;5;241m=\u001B[39m load_pretrained_model_and_tokenizer(pretrained_model_name_or_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbert-base-uncased\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      6\u001B[0m                                                        return_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m      7\u001B[0m                                                        use_cuda \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m----> 9\u001B[0m word_embedder \u001B[38;5;241m=\u001B[39m \u001B[43mWordEmbedder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m110\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\transformers_keras_dataloader\\scripts\\WordEmbedder.py:75\u001B[0m, in \u001B[0;36mWordEmbedder.__init__\u001B[1;34m(self, max_length, oov, infer_oov_after_embed, all_special_tokens, pad_token_id, pooling_layer_number, policy_dict, common_params, model_x_vars_params, is_tokenizer_fast, use_gpu, call_by_dataloader)\u001B[0m\n\u001B[0;32m     73\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moffset_mapper \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     74\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder_params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m---> 75\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_params\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     77\u001B[0m   \u001B[38;5;66;03m# Since this a word embedder we need an algorithm to map tokenized indices to reference tokens(text split using split())\u001B[39;00m\n\u001B[0;32m     78\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moffset_mapper \u001B[38;5;241m=\u001B[39m OffsetMapper(offset_config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_x_vars_params[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moffset_mapper_config\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m     79\u001B[0m                                     all_special_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mall_special_tokens)\n",
      "File \u001B[1;32m~\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\transformers_keras_dataloader\\scripts\\WordEmbedder.py:230\u001B[0m, in \u001B[0;36mWordEmbedder._get_params\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    228\u001B[0m \u001B[38;5;66;03m# model_vars_params\u001B[39;00m\n\u001B[0;32m    229\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(model_vars_params_config_path) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m--> 230\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_vars_params_dict \u001B[38;5;241m=\u001B[39m \u001B[43mjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\json\\__init__.py:293\u001B[0m, in \u001B[0;36mload\u001B[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n\u001B[0;32m    274\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(fp, \u001B[38;5;241m*\u001B[39m, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, object_hook\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, parse_float\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    275\u001B[0m         parse_int\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, parse_constant\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, object_pairs_hook\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw):\n\u001B[0;32m    276\u001B[0m     \u001B[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001B[39;00m\n\u001B[0;32m    277\u001B[0m \u001B[38;5;124;03m    a JSON document) to a Python object.\u001B[39;00m\n\u001B[0;32m    278\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    291\u001B[0m \u001B[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001B[39;00m\n\u001B[0;32m    292\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 293\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loads(\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m    294\u001B[0m         \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mcls\u001B[39m, object_hook\u001B[38;5;241m=\u001B[39mobject_hook,\n\u001B[0;32m    295\u001B[0m         parse_float\u001B[38;5;241m=\u001B[39mparse_float, parse_int\u001B[38;5;241m=\u001B[39mparse_int,\n\u001B[0;32m    296\u001B[0m         parse_constant\u001B[38;5;241m=\u001B[39mparse_constant, object_pairs_hook\u001B[38;5;241m=\u001B[39mobject_pairs_hook, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\encodings\\cp1252.py:23\u001B[0m, in \u001B[0;36mIncrementalDecoder.decode\u001B[1;34m(self, input, final)\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, final\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m---> 23\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcodecs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcharmap_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdecoding_table\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m: 'charmap' codec can't decode byte 0x81 in position 317: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "from transformers_keras_dataloader import load_pretrained_model_and_tokenizer\n",
    "from transformers_keras_dataloader import WordEmbedder\n",
    "\n",
    "\n",
    "tokenizer, model = load_pretrained_model_and_tokenizer(pretrained_model_name_or_path = \"bert-base-uncased\",\n",
    "                                                       return_model = True,\n",
    "                                                       use_cuda = True)\n",
    "\n",
    "word_embedder = WordEmbedder(max_length=110)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset\n",
    "id_train=[0,1,2]\n",
    "x_train=[ \"Dies\",'ist','eine',\"Kirche\",'elefantenscheißegewitter']\n",
    "y_train=[ \"A\",\"B\",\"B\",\"B\",\"B\"]\n",
    "x_train=[\"Auto\",'Bahn','elefantenscheißegewitter']\n",
    "y_train=[ \"B\",\"B\",\"C\"]\n",
    "d = {'train':Dataset.from_dict({'id':id_train,'tokens':x_train,'ner_tags':y_train}),\n",
    "     'val':Dataset.from_dict({'id':id_train,'tokens':x_train,'ner_tags':y_train})#,\n",
    "     #'test':Dataset.from_dict({'label':y_test,'text':x_test})\n",
    "     }\n",
    "\n",
    "dataset_testen=DatasetDict(d)\n",
    "\n",
    "tokenized_inputs = tokenizer(\n",
    "        dataset_testen[\"train\"]['tokens'], truncation=True, is_split_into_words=False\n",
    "    )\n",
    "labels = []\n",
    "for i, label in enumerate(dataset_testen[\"train\"][f\"ner_tags\"]):\n",
    "        # get a list of tokens their connecting word id (for words tokenized into multiple chunks)\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to the current\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "\n",
    "tokenized_inputs[\"labels\"] = labels\n",
    "\n",
    "dataset=tokenized_inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:248\u001B[0m, in \u001B[0;36mBatchEncoding.__getattr__\u001B[1;34m(self, item)\u001B[0m\n\u001B[0;32m    247\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 248\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n",
      "\u001B[1;31mKeyError\u001B[0m: 'features'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [79], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m pre_tokenizer_columns \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeatures\u001B[49m)\n\u001B[0;32m      2\u001B[0m tokenizer_columns \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mset\u001B[39m(dataset\u001B[38;5;241m.\u001B[39mfeatures) \u001B[38;5;241m-\u001B[39m pre_tokenizer_columns)\n",
      "File \u001B[1;32m~\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:250\u001B[0m, in \u001B[0;36mBatchEncoding.__getattr__\u001B[1;34m(self, item)\u001B[0m\n\u001B[0;32m    248\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[item]\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n\u001B[1;32m--> 250\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "pre_tokenizer_columns = set(dataset.features)\n",
    "tokenizer_columns = list(set(dataset.features) - pre_tokenizer_columns)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:248\u001B[0m, in \u001B[0;36mBatchEncoding.__getattr__\u001B[1;34m(self, item)\u001B[0m\n\u001B[0;32m    247\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 248\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n",
      "\u001B[1;31mKeyError\u001B[0m: 'to_tf_dataset'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [81], line 9\u001B[0m\n\u001B[0;32m      4\u001B[0m data_collator \u001B[38;5;241m=\u001B[39m DataCollatorForTokenClassification(\n\u001B[0;32m      5\u001B[0m     tokenizer\u001B[38;5;241m=\u001B[39mtokenizer, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      6\u001B[0m )\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# converting our train dataset to tf.data.Dataset\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m tf_train_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_tf_dataset\u001B[49m(\n\u001B[0;32m     10\u001B[0m     columns\u001B[38;5;241m=\u001B[39m tokenizer_columns,\n\u001B[0;32m     11\u001B[0m     shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m     12\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m,\n\u001B[0;32m     13\u001B[0m     collate_fn\u001B[38;5;241m=\u001B[39mdata_collator,\n\u001B[0;32m     14\u001B[0m )\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# converting our test dataset to tf.data.Dataset\u001B[39;00m\n\u001B[0;32m     17\u001B[0m tf_eval_dataset \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mto_tf_dataset(\n\u001B[0;32m     18\u001B[0m     columns\u001B[38;5;241m=\u001B[39mtokenizer_columns,\n\u001B[0;32m     19\u001B[0m     shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m     20\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m,\n\u001B[0;32m     21\u001B[0m     collate_fn\u001B[38;5;241m=\u001B[39mdata_collator,\n\u001B[0;32m     22\u001B[0m )\n",
      "File \u001B[1;32m~\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:250\u001B[0m, in \u001B[0;36mBatchEncoding.__getattr__\u001B[1;34m(self, item)\u001B[0m\n\u001B[0;32m    248\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[item]\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n\u001B[1;32m--> 250\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# Data collator that will dynamically pad the inputs received, as well as the labels.\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer, return_tensors=\"tf\"\n",
    ")\n",
    "\n",
    "# converting our train dataset to tf.data.Dataset\n",
    "tf_train_dataset = dataset.to_tf_dataset(\n",
    "    columns= tokenizer_columns,\n",
    "    shuffle=False,\n",
    "    batch_size=100,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "# converting our test dataset to tf.data.Dataset\n",
    "tf_eval_dataset = dataset.to_tf_dataset(\n",
    "    columns=tokenizer_columns,\n",
    "    shuffle=False,\n",
    "    batch_size=100,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "#Download the pretrained transformer model and fine-tune it.\n",
    "from transformers import TFAutoModelForTokenClassification, create_optimizer\n",
    "\n",
    "\n",
    "num_train_steps = len(tf_train_dataset) * 10\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=0.001,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.001,\n",
    "    num_warmup_steps=3,\n",
    ")\n",
    "\n",
    "id2label = {str(i): label for i, label in enumerate(y_train)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "     \"deepset/gbert-base\",\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "model.compile(optimizer=optimizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:248\u001B[0m, in \u001B[0;36mBatchEncoding.__getattr__\u001B[1;34m(self, item)\u001B[0m\n\u001B[0;32m    247\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 248\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n",
      "\u001B[1;31mKeyError\u001B[0m: 'features'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [80], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeatures\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:250\u001B[0m, in \u001B[0;36mBatchEncoding.__getattr__\u001B[1;34m(self, item)\u001B[0m\n\u001B[0;32m    248\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[item]\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n\u001B[1;32m--> 250\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "dataset.features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "{'text_a': <tf.Tensor: shape=(), dtype=string, numpy=b'Dies ist ein Test'>,\n 'text_b': <tf.Tensor: shape=(), dtype=string, numpy=b'Kartoffelsalat ist sehr geil'>}"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "examples = {\n",
    "    \"text_a\": [\n",
    "      \"Dies ist ein Test\",\n",
    "      \"Der Mann geht zur Kirche\"\n",
    "    ],\n",
    "    \"text_b\": [\n",
    "     \"Kartoffelsalat ist sehr geil\",\n",
    "     \"Mir ist eine Kirche zugelaufen.\"\n",
    "  ],\n",
    "}\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(examples)\n",
    "next(iter(dataset))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "model_id = \"deepset/gbert-base\"\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/26200 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f0b87e2f1cd4a0bb7fcb5d9a852d601"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [49], line 30\u001B[0m\n\u001B[0;32m     27\u001B[0m     tokenized_inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m labels\n\u001B[0;32m     28\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokenized_inputs\n\u001B[1;32m---> 30\u001B[0m tokenized_datasets \u001B[38;5;241m=\u001B[39m \u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenize_and_align_labels\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\datasets\\dataset_dict.py:777\u001B[0m, in \u001B[0;36mDatasetDict.map\u001B[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001B[0m\n\u001B[0;32m    774\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cache_file_names \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    775\u001B[0m     cache_file_names \u001B[38;5;241m=\u001B[39m {k: \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m}\n\u001B[0;32m    776\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DatasetDict(\n\u001B[1;32m--> 777\u001B[0m     {\n\u001B[0;32m    778\u001B[0m         k: dataset\u001B[38;5;241m.\u001B[39mmap(\n\u001B[0;32m    779\u001B[0m             function\u001B[38;5;241m=\u001B[39mfunction,\n\u001B[0;32m    780\u001B[0m             with_indices\u001B[38;5;241m=\u001B[39mwith_indices,\n\u001B[0;32m    781\u001B[0m             with_rank\u001B[38;5;241m=\u001B[39mwith_rank,\n\u001B[0;32m    782\u001B[0m             input_columns\u001B[38;5;241m=\u001B[39minput_columns,\n\u001B[0;32m    783\u001B[0m             batched\u001B[38;5;241m=\u001B[39mbatched,\n\u001B[0;32m    784\u001B[0m             batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m    785\u001B[0m             drop_last_batch\u001B[38;5;241m=\u001B[39mdrop_last_batch,\n\u001B[0;32m    786\u001B[0m             remove_columns\u001B[38;5;241m=\u001B[39mremove_columns,\n\u001B[0;32m    787\u001B[0m             keep_in_memory\u001B[38;5;241m=\u001B[39mkeep_in_memory,\n\u001B[0;32m    788\u001B[0m             load_from_cache_file\u001B[38;5;241m=\u001B[39mload_from_cache_file,\n\u001B[0;32m    789\u001B[0m             cache_file_name\u001B[38;5;241m=\u001B[39mcache_file_names[k],\n\u001B[0;32m    790\u001B[0m             writer_batch_size\u001B[38;5;241m=\u001B[39mwriter_batch_size,\n\u001B[0;32m    791\u001B[0m             features\u001B[38;5;241m=\u001B[39mfeatures,\n\u001B[0;32m    792\u001B[0m             disable_nullable\u001B[38;5;241m=\u001B[39mdisable_nullable,\n\u001B[0;32m    793\u001B[0m             fn_kwargs\u001B[38;5;241m=\u001B[39mfn_kwargs,\n\u001B[0;32m    794\u001B[0m             num_proc\u001B[38;5;241m=\u001B[39mnum_proc,\n\u001B[0;32m    795\u001B[0m             desc\u001B[38;5;241m=\u001B[39mdesc,\n\u001B[0;32m    796\u001B[0m         )\n\u001B[0;32m    797\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m k, dataset \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m    798\u001B[0m     }\n\u001B[0;32m    799\u001B[0m )\n",
      "File \u001B[1;32m~\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\datasets\\dataset_dict.py:778\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    774\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cache_file_names \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    775\u001B[0m     cache_file_names \u001B[38;5;241m=\u001B[39m {k: \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m}\n\u001B[0;32m    776\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DatasetDict(\n\u001B[0;32m    777\u001B[0m     {\n\u001B[1;32m--> 778\u001B[0m         k: \u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    779\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfunction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    780\u001B[0m \u001B[43m            \u001B[49m\u001B[43mwith_indices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwith_indices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    781\u001B[0m \u001B[43m            \u001B[49m\u001B[43mwith_rank\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwith_rank\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    782\u001B[0m \u001B[43m            \u001B[49m\u001B[43minput_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_columns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    783\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbatched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatched\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    784\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    785\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdrop_last_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdrop_last_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    786\u001B[0m \u001B[43m            \u001B[49m\u001B[43mremove_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mremove_columns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    787\u001B[0m \u001B[43m            \u001B[49m\u001B[43mkeep_in_memory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_in_memory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    788\u001B[0m \u001B[43m            \u001B[49m\u001B[43mload_from_cache_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mload_from_cache_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    789\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcache_file_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_file_names\u001B[49m\u001B[43m[\u001B[49m\u001B[43mk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    790\u001B[0m \u001B[43m            \u001B[49m\u001B[43mwriter_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwriter_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    791\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    792\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdisable_nullable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable_nullable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    793\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfn_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfn_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    794\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnum_proc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_proc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    795\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdesc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdesc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    796\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    797\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m k, dataset \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m    798\u001B[0m     }\n\u001B[0;32m    799\u001B[0m )\n",
      "File \u001B[1;32m~\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:2585\u001B[0m, in \u001B[0;36mDataset.map\u001B[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[0m\n\u001B[0;32m   2582\u001B[0m disable_tqdm \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m logging\u001B[38;5;241m.\u001B[39mis_progress_bar_enabled()\n\u001B[0;32m   2584\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_proc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m num_proc \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m-> 2585\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_single\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2586\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2587\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwith_indices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwith_indices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2588\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwith_rank\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwith_rank\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2589\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_columns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2590\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatched\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2591\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2592\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdrop_last_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdrop_last_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2593\u001B[0m \u001B[43m        \u001B[49m\u001B[43mremove_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mremove_columns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2594\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_in_memory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_in_memory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2595\u001B[0m \u001B[43m        \u001B[49m\u001B[43mload_from_cache_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mload_from_cache_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2596\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_file_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_file_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2597\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwriter_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwriter_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2598\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2599\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisable_nullable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable_nullable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2600\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfn_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfn_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2601\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnew_fingerprint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnew_fingerprint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2602\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisable_tqdm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable_tqdm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2603\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdesc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdesc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2604\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2605\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2607\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mformat_cache_file_name\u001B[39m(cache_file_name, rank):\n",
      "File \u001B[1;32m~\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:585\u001B[0m, in \u001B[0;36mtransmit_tasks.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    583\u001B[0m     \u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    584\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[1;32m--> 585\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    586\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[0;32m    587\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m datasets:\n\u001B[0;32m    588\u001B[0m     \u001B[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:552\u001B[0m, in \u001B[0;36mtransmit_format.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    545\u001B[0m self_format \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    546\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type,\n\u001B[0;32m    547\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_kwargs,\n\u001B[0;32m    548\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_columns,\n\u001B[0;32m    549\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_all_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_all_columns,\n\u001B[0;32m    550\u001B[0m }\n\u001B[0;32m    551\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[1;32m--> 552\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    553\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[0;32m    554\u001B[0m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\datasets\\fingerprint.py:480\u001B[0m, in \u001B[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    476\u001B[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001B[0;32m    478\u001B[0m \u001B[38;5;66;03m# Call actual function\u001B[39;00m\n\u001B[1;32m--> 480\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    482\u001B[0m \u001B[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001B[39;00m\n\u001B[0;32m    484\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inplace:  \u001B[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:2967\u001B[0m, in \u001B[0;36mDataset._map_single\u001B[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001B[0m\n\u001B[0;32m   2965\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m batched:\n\u001B[0;32m   2966\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, example \u001B[38;5;129;01min\u001B[39;00m pbar:\n\u001B[1;32m-> 2967\u001B[0m         example \u001B[38;5;241m=\u001B[39m \u001B[43mapply_function_on_filtered_inputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moffset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2968\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m update_data:\n\u001B[0;32m   2969\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:2865\u001B[0m, in \u001B[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001B[1;34m(inputs, indices, check_same_num_examples, offset)\u001B[0m\n\u001B[0;32m   2863\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m with_rank:\n\u001B[0;32m   2864\u001B[0m     additional_args \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (rank,)\n\u001B[1;32m-> 2865\u001B[0m processed_inputs \u001B[38;5;241m=\u001B[39m function(\u001B[38;5;241m*\u001B[39mfn_args, \u001B[38;5;241m*\u001B[39madditional_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfn_kwargs)\n\u001B[0;32m   2866\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m update_data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   2867\u001B[0m     \u001B[38;5;66;03m# Check if the function returns updated examples\u001B[39;00m\n\u001B[0;32m   2868\u001B[0m     update_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28misinstance\u001B[39m(processed_inputs, (Mapping, pa\u001B[38;5;241m.\u001B[39mTable))\n",
      "File \u001B[1;32m~\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:2545\u001B[0m, in \u001B[0;36mDataset.map.<locals>.decorate.<locals>.decorated\u001B[1;34m(item, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2541\u001B[0m decorated_item \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2542\u001B[0m     Example(item, features\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m batched \u001B[38;5;28;01melse\u001B[39;00m Batch(item, features\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures)\n\u001B[0;32m   2543\u001B[0m )\n\u001B[0;32m   2544\u001B[0m \u001B[38;5;66;03m# Use the LazyDict internally, while mapping the function\u001B[39;00m\n\u001B[1;32m-> 2545\u001B[0m result \u001B[38;5;241m=\u001B[39m f(decorated_item, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   2546\u001B[0m \u001B[38;5;66;03m# Return a standard dict\u001B[39;00m\n\u001B[0;32m   2547\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(result, LazyDict) \u001B[38;5;28;01melse\u001B[39;00m result\n",
      "Cell \u001B[1;32mIn [49], line 19\u001B[0m, in \u001B[0;36mtokenize_and_align_labels\u001B[1;34m(examples)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# We set the label for the first token of each word.\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m word_idx \u001B[38;5;241m!=\u001B[39m previous_word_idx:\n\u001B[1;32m---> 19\u001B[0m     label_ids\u001B[38;5;241m.\u001B[39mappend(\u001B[43mlabel\u001B[49m\u001B[43m[\u001B[49m\u001B[43mword_idx\u001B[49m\u001B[43m]\u001B[49m)\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# For the other tokens in a word, we set the label to the current\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     22\u001B[0m     label_ids\u001B[38;5;241m.\u001B[39mappend(label[word_idx])\n",
      "\u001B[1;31mTypeError\u001B[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        # get a list of tokens their connecting word id (for words tokenized into multiple chunks)\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to the current\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DatasetDict' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [40], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdataset_testen\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: 'DatasetDict' object is not an iterator"
     ]
    }
   ],
   "source": [
    "next(dataset_testen)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset germaner (C:/Users/Timo/.cache/huggingface/datasets/germaner/default/0.9.1/98610f255094d6f67f37c379e5e9f0800322705df916299ddd09ac6dab80bbe8)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe2fb9ba72ca44878b33708187b13790"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_id=\"germaner\"\n",
    "dataset = load_dataset(dataset_id)\n",
    "\n",
    "# accessing the \"train\" split for the \"ner_tags\" feature\n",
    "ner_labels = dataset[\"train\"].features[\"ner_tags\"].feature.names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'ner_tags'],\n        num_rows: 26200\n    })\n})"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [[102, 2510, 103], [102, 1974, 103], [102, 1846, 2651, 2132, 4539, 790, 1313, 2547, 103]], 'token_type_ids': [[0, 0, 0], [0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_inputs\n",
    "#[102, 2510, 1974, 103]\n",
    "#{'input_ids': [102, 1406, 215, 261, 2048, 1846, 2651, 2132, 4539, 790, 1313, 2547, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_embedder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [18], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Generate word embeddings for X\u001B[39;00m\n\u001B[0;32m      2\u001B[0m x\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapple\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m----> 3\u001B[0m word_embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mword_embedder\u001B[49m\u001B[38;5;241m.\u001B[39mprepare_embeddings(X\u001B[38;5;241m=\u001B[39mx, tokenizer\u001B[38;5;241m=\u001B[39mtokenizer, model\u001B[38;5;241m=\u001B[39mmodel)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'word_embedder' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate word embeddings for X\n",
    "x=[\"apple\"]\n",
    "word_embeddings = word_embedder.prepare_embeddings(X=x, tokenizer=tokenizer, model=model)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    conll['train'].map(tokenize_and_align_labels, batched=True),\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    conll['validation'].map(tokenize_and_align_labels, batched=True),\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index_from' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [9], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# defines the number of visualized words\u001B[39;00m\n\u001B[0;32m      8\u001B[0m num_visualizations \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m250\u001B[39m\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[43mindex_from\u001B[49m \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m offset,\n\u001B[0;32m     11\u001B[0m                index_from \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m offset \u001B[38;5;241m+\u001B[39m num_visualizations):\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(embedding_layer\u001B[38;5;241m.\u001B[39mget_weights()[\u001B[38;5;241m0\u001B[39m]):\n\u001B[0;32m     13\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'index_from' is not defined"
     ]
    }
   ],
   "source": [
    "# Select words that will be visualized\n",
    "labels = []\n",
    "tokens = []\n",
    "\n",
    "# offset can be set to skip stop words\n",
    "offset = 100\n",
    "# defines the number of visualized words\n",
    "num_visualizations = 250\n",
    "\n",
    "for i in range(index_from + 1 + offset,\n",
    "               index_from + 1 + offset + num_visualizations):\n",
    "    if i >= len(embedding_layer.get_weights()[0]):\n",
    "        break\n",
    "    tokens.append(embedding_layer.get_weights()[0][i])\n",
    "    labels.append(inverted_word_index[i])\n",
    "\n",
    "# use t-SNE as dimension reduction method\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500,random_state=42)\n",
    "new_values = tsne_model.fit_transform(np.asarray(tokens))\n",
    "\n",
    "# visualize data\n",
    "x = []\n",
    "y = []\n",
    "for value in new_values:\n",
    "    x.append(value[0])\n",
    "    y.append(value[1])\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "for i in range(len(x)):\n",
    "    plt.scatter(x[i], y[i])\n",
    "    plt.annotate(labels[i],\n",
    "                 xy=(x[i], y[i]),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}