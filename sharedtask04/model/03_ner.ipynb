{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, TFAutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from transformers import create_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CoNLL 2003 Dataset\n",
    "![](images/conll2003.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset conll2003 (C:/Users/Timo/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "486b4cab899f49cbab0c0ec73f4e5934"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 14041\n    })\n    validation: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 3250\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 3453\n    })\n})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll = load_dataset('conll2003')\n",
    "conll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daten liegen im CoNLL-Format vor und enthalten neben Tokens und Named Entities auch POS Tags und Phrasenannotationen.\n",
    "![](images/conll-format.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens des ersten Trainingsatzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll['train'][0]['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tags des ersten Trainingssatzes\n",
    "Es wird das [Penn Treebank Tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) genutzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[22, 42, 16, 21, 35, 37, 16, 21, 7]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll['train'][0]['pos_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Sequence(feature=ClassLabel(names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll['train'].features['pos_tags']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es wird das BIO-Encoding-Schema genutzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[3, 0, 7, 0, 0, 0, 7, 0, 0]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll['train'][0]['ner_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll['train'].features['ner_tags']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "Wir nutzen das Language Model [distilbert-base-cased](https://huggingface.co/distilbert-base-cased)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Model Tokenizer tokenisieren Text in Tokens und Subtokens. Falls der zu verarbeitende Text bereits tokenisiert vorliegt, dann sollte diese Tokenisierung für NER beibehalten werden. Eine andere Tokenisierung würde dazu führen, dass Tokengrenzen nicht mehr mit den Named Entity Labels übereinstimmen.\n",
    "Ein erneutes Tokenisieren verhindern wir durch den Parameter ```is_split_into_words=True```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['[CLS]',\n 'EU',\n 'rejects',\n 'German',\n 'call',\n 'to',\n 'boycott',\n 'British',\n 'la',\n '##mb',\n '.',\n '[SEP]']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(conll['train'][0]['tokens'], is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity labels müssen korrigiert werden: nur das erste Subtoken je Token wird mit einem Label versehen, weitere Subtokens werden ignoriert ([https://huggingface.co/docs/transformers/tasks/token_classification](https://huggingface.co/docs/transformers/tasks/token_classification))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(sequence_batch):\n",
    "    # tokenize pre-tokenized sequences\n",
    "    # long sequences will be truncated to respect the maximum token length of the language model (usually 512)\n",
    "    tokenized_sequences = tokenizer(sequence_batch['tokens'], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    # iterate over pre-tokenized tokens of single sequences\n",
    "    for i, label in enumerate(sequence_batch['ner_tags']):\n",
    "        # get associated word ids of the subtokens\n",
    "        # if a token has multiple subtokens, then each subtoken is associated with the token's word id\n",
    "        word_ids = tokenized_sequences.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        # iterate over subtokens\n",
    "        for word_idx in word_ids:\n",
    "            # special tokens (e.g. [CLS], [SEP]) get label id -100 -> loss function will ignore them\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # if the first subtoken of the next token is encountered, then associate the token's ner label with the subtoken\n",
    "            # FIXME what if two consecutive tokens are identical (e.g. \"is this a really really bad?\")?\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # consecutive subtokens will be ignored\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            # memorize the current word\n",
    "            previous_word_idx = word_idx\n",
    "        # add labels of the current sequence to the list of labels of the batch\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    # update batch labels\n",
    "    tokenized_sequences['labels'] = labels\n",
    "    return tokenized_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and training definition\n",
    "Load auto model for token classification. This adds a classification layer / head to the language model (usually a simple dense layer and dropout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForTokenClassification: ['vocab_transform', 'vocab_projector', 'activation_13', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['dropout_19', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForTokenClassification.from_pretrained(model_name, num_labels=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify training parameters and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_token_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 65190912  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  6921      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,197,833\n",
      "Trainable params: 6,921\n",
      "Non-trainable params: 65,190,912\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "num_train_epochs = 1\n",
    "num_train_steps = (len(conll['train']) // batch_size) * num_train_epochs\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    "    num_warmup_steps=0,\n",
    ")\n",
    "\n",
    "model.get_layer('distilbert').trainable=False\n",
    "\n",
    "model.compile(optimizer=optimizer, metrics='acc')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init collator to build batches, pad sequences in a batch etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:/Users/Timo/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98\\cache-1180f3410eff14d3.arrow\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:715: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tensor = as_tensor(value)\n",
      "Loading cached processed dataset at C:/Users/Timo/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98\\cache-698bbe1a45034a69.arrow\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    conll['train'].map(tokenize_and_align_labels, batched=True),\n",
    "    shuffle=True,\n",
    "    batch_size=1,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    conll['validation'].map(tokenize_and_align_labels, batched=True),\n",
    "    shuffle=False,\n",
    "    batch_size=1,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nDetected at node 'tf_distil_bert_for_token_classification/Relu' defined at (most recent call last):\n    File \"C:\\Users\\Timo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\Timo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 982, in launch_instance\n      app.start()\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\Timo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\Timo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\Timo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Timo\\AppData\\Local\\Temp\\ipykernel_6156\\3530048125.py\", line 3, in <module>\n      model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=num_train_epochs)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1495, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 829, in run_call_with_unpacked_inputs\n      try:\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py\", line 849, in call\n      loss = None if labels is None else self.hf_compute_loss(labels, logits)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py\", line 849, in call\n      loss = None if labels is None else self.hf_compute_loss(labels, logits)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 252, in hf_compute_loss\n      if self.config.tf_legacy_loss:\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 266, in hf_compute_loss\n      unmasked_loss = loss_fn(tf.nn.relu(labels), logits)\nNode: 'tf_distil_bert_for_token_classification/Relu'\nJIT compilation failed.\n\t [[{{node tf_distil_bert_for_token_classification/Relu}}]] [Op:__inference_train_function_14373]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnknownError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [15], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m      2\u001B[0m os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mXLA_FLAGS\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--xla_gpu_cuda_data_dir=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.8\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 3\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtf_train_set\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtf_validation_set\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_train_epochs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_Execute(ctx\u001B[38;5;241m.\u001B[39m_handle, device_name, op_name,\n\u001B[0;32m     55\u001B[0m                                       inputs, attrs, num_outputs)\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mUnknownError\u001B[0m: Graph execution error:\n\nDetected at node 'tf_distil_bert_for_token_classification/Relu' defined at (most recent call last):\n    File \"C:\\Users\\Timo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\Timo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 982, in launch_instance\n      app.start()\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\Timo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\Timo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\Timo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Timo\\AppData\\Local\\Temp\\ipykernel_6156\\3530048125.py\", line 3, in <module>\n      model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=num_train_epochs)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1495, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 829, in run_call_with_unpacked_inputs\n      try:\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py\", line 849, in call\n      loss = None if labels is None else self.hf_compute_loss(labels, logits)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py\", line 849, in call\n      loss = None if labels is None else self.hf_compute_loss(labels, logits)\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 252, in hf_compute_loss\n      if self.config.tf_legacy_loss:\n    File \"C:\\Users\\Timo\\PycharmProjects\\shared-tasks-wintersemester-2022-23\\venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 266, in hf_compute_loss\n      unmasked_loss = loss_fn(tf.nn.relu(labels), logits)\nNode: 'tf_distil_bert_for_token_classification/Relu'\nJIT compilation failed.\n\t [[{{node tf_distil_bert_for_token_classification/Relu}}]] [Op:__inference_train_function_14373]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"XLA_FLAGS\"]=\"--xla_gpu_cuda_data_dir=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.8\"\n",
    "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=num_train_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}